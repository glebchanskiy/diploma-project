\section{Разработка интеллектуальной системы управления потоками пользовательских обращений}
\label{sec:development}

Данная глава посвящена описанию процесса разработки, начиная от выбора технологического стека и заканчивая имплементацией основных модулей и механизмов, которые воплощают в жизнь спроектированную концепцию.

В ходе изложения будут рассмотрены технические аспекты создания универсального ядра системы, способного осуществлять интеллектуальный навигационный поиск по иерархической базе знаний. Особое внимание будет уделено реализации алгоритма последовательного углубления, интеграции с языковыми моделями (LLM) в роли ассистентов-навигаторов, а также механизмам формирования объяснимых и структурированных ответов. Кроме того, будет продемонстрировано, как это ядро интегрируется с пользовательскими интерфейсами, на примере Telegram-бота, и как обеспечивается управление знаниями и контекстом взаимодействия. Цель данной главы – не только представить конечный программный продукт, но и раскрыть ключевые инженерные решения, принятые в процессе его создания.

\subsection{Общие сведения о реализации}

Практическая реализация интеллектуальной системы управления потоками пользовательских обращений потребовала выбора современного и гибкого технологического стека, а также четкой организации структуры проекта для обеспечения модульности и дальнейшего развития.

Для разработки ядра системы и сопутствующих компонентов был выбран язык TypeScript в связке со средой выполнения Deno. Такое решение обусловлено преимуществами строгой типизации TypeScript, что критически важно для создания надежных и сложных систем, а также современными возможностями Deno, включая встроенную поддержку TypeScript, безопасность и эффективное управление модулями.

Центральным элементом хранения знаний выступает графовая база данных Neo4j. Ее выбор продиктован естественной способностью представлять иерархические и сложносвязанные данные, что идеально соответствует концепции дерева знаний системы. Взаимодействие с Neo4j из кода приложения реализовано посредством официального JavaScript-драйвера.

Интеллектуальная составляющая, отвечающая за навигацию по базе знаний и анализ запросов, опирается на большие языковые модели (LLM), доступ к которым осуществляется через API OpenAI-совместимых сервисов, в частности, с использованием модели deepseek-chat. Для интеграции с LLM используется соответствующая JavaScript/TypeScript библиотека.

В качестве демонстрационного пользовательского интерфейса и канала взаимодействия с системой был разработан Telegram-бот. Его реализация выполнена с помощью популярной библиотеки grammy, предоставляющей удобные инструменты для создания ботов. Управление зависимостями проекта и его сборка осуществляются встроенными средствами Deno. Для обеспечения портативности и стандартизации окружения базы данных Neo4j используется Docker.

\pic[14cm]{images/core-tg-adapter.png}{Схема реализованного ядра системы}{core-tg-adapter}

Структура исходного кода проекта была продумана для обеспечения модульности и четкого разделения ответственности. Центральным компонентом является engine, который инкапсулирует все ядро, включая логику взаимодействия с LLM, графовой базой данных Neo4j, управление промптами и основной класс Context, управляющий сессией поиска. Внешниq интерфейс, реализован на примере адаптера для Telegram-бота, обеспечивает взаимодействие пользователя с ядром, рисунок \ref{core-tg-adapter}. 


\subsection{Реализация архитектуры системы}

При разработке интеллектуальной системы особое внимание уделялось созданию гибкой и масштабируемой архитектуры. На текущем этапе реализации (MVP) система состоит из центрального интеллектуального ядра и одного пользовательского адаптера (Telegram-бот). Ядро инкапсулирует всю основную логику обработки запросов, взаимодействия с базой знаний и языковыми моделями. Такая модульная архитектура, даже в рамках текущей монолитной реализации ядра, позволяет четко разделить зоны ответственности и упрощает дальнейшее развитие. В перспективе, при росте нагрузок и функциональных требований, отдельные компоненты ядра или система целиком могут быть трансформированы в микросервисную архитектуру, где каждый сервис будет отвечать за свою узкоспециализированную задачу (например, сервис управления базой знаний, сервис оркестрации LLM-взаимодействий, сервис обработки пользовательских сессий).

\subsubsection{Структура реализованных модулей и перспективы микросервисной архитектуры}

В текущей реализации MVP ключевым программным модулем является интеллектуальное ядро. Этот модуль является сердцем системы и отвечает за управление сессией обработки запроса. Ядро также включает компоненты для взаимодействия с графовой базой данных Neo4j, необходимые для извлечения фрагментов знаний и модули для формирования запросов (промптов) к языковой модели и обработки ее ответов с целью навигации по базе знаний. Кроме того, ядро ведет историю взаимодействия и контекста в рамках одной сессии. Вторым важным модулем является адаптер Telegram-бота, который служит точкой входа для пользователей. Он принимает сообщения, инициализирует сессию обработки запроса ядра и передает итоговый ответ от ядра обратно пользователю.

В более развитой, микросервисной архитектуре, концепция которой была рассмотрена на этапе проектирования, эти функции были бы распределены между несколькими независимыми сервисами. Например, "Модуль запросов" и "Модуль команд" могли бы функционировать как отдельные микросервисы, взаимодействующие с ядром, которое, в свою очередь, также могло бы быть разделено на сервис поиска решений и сервис управления знаниями. "Пользовательские адаптеры" для различных каналов, таких как Telegram, веб-интерфейсы или электронная почта, также представляли бы собой отдельные, независимо развертываемые сервисы.

\subsubsection{Организация асинхронного взаимодействия}

В текущей реализации ядра асинхронность является фундаментальным аспектом и достигается за счет активного использования конструкций async/await в TypeScript. Это необходимо для обеспечения неблокирующего взаимодействия с внешними системами, такими как база данных Neo4j и API языковой модели, ответы от которых могут приходить с задержкой. Методы, отвечающие за выполнение поисковых итераций, запросы к LLM и операции с базой данных, такие как Context. Все методы реализованы как асинхронные. Это позволяет эффективно обрабатывать запросы, не замораживая основной поток выполнения приложения и обеспечивая отзывчивость системы.

Для полномасштабной, распределенной системы, как было описано в разделе ранее, организация асинхронного взаимодействия между микросервисами потребует дополнительного внедрения брокера сообщений, например, Apache Kafka. Такое решение обеспечит надежную доставку сообщений между сервисами, их развязку (когда сервисы-продюсеры не зависят напрямую от сервисов-консьюмеров), а также возможности для масштабирования за счет параллельной обработки сообщений и повышения отказоустойчивости. В подобной архитектуре могли бы использоваться специализированные топики Kafka для различных типов сообщений, таких как входящие запросы пользователей, задачи для LLM, события обновления базы знаний, логирование событий сессий и передача финальных ответов. Хотя в текущем MVP Kafka не используется, заложенные в ядре асинхронные операции являются прочной основой для будущей интеграции с подобными системами обмена сообщениями при переходе к микросервисной архитектуре.

\subsubsection{Реализация универсального API взаимодействия}

Для обеспечения легкой интеграции интеллектуального ядра с разнообразными пользовательскими интерфейсами был реализован унифицированный программный API. Центральным элементом этого API является фабричный метод ContextFactory.createRequest(), который инициализирует сессию обработки запроса, возвращая объект Context. Через публичные методы этого объекта любой внешний адаптер, будь то Telegram-бот, веб-приложение или другой сервис, может единообразно управлять процессом  поиска и получать структурированные результаты. Такой подход значительно упрощает создание новых адаптеров и обеспечивает гибкость системы, закладывая основу для возможного перехода к более формальному сетевому API (например, REST или gRPC) в случае выделения ядра в отдельный микросервис в будущем.

\subsection{Реализация подсистемы управления знаниями}

Функционирование интеллектуальной системы неразрывно связано с эффективным управлением базой знаний (БЗ) и контекстом взаимодействия в процессе поиска. Качество и структура БЗ напрямую определяют релевантность и точность ответов, предоставляемых системой. Данный подраздел описывает ключевые аспекты реализации хранилища знаний и механизмов управления контекстной информацией.

\subsubsection{Модель хранения иерархической базы знаний}

Центральным элементом подсистемы управления знаниями является иерархическая база данных, для реализации которой была выбрана графовая СУБД Neo4j. Такой выбор обусловлен нативной поддержкой Neo4j графовых моделей данных, что идеально соответствует концепции представления знаний в виде дерева или более сложного графа. Язык запросов Cypher, используемый в Neo4j, оптимизирован для эффективной навигации по связям между узлами, что является критически важным для реализации алгоритма последовательного углубления, применяемого в ядре системы. Гибкость схемы Neo4j также позволяет легко модифицировать и расширять структуру БЗ по мере необходимости.

\pic[17cm]{images/screen-n4-classes.png}{Пример фрагмента графа знаний в Neo4j, иллюстрирующий узлы Element и связи :DECOMPOSITION для нескольких связанных понятий}{screen-n4-classes}

В реализованной модели знания представляются в виде узлов с меткой Element. Каждый такой узел обладает набором свойств, ключевыми из которых являются уникальный идентификатор (внутренний ID Neo4j), name (наименование или заголовок фрагмента знания) и description (детальное текстовое содержание). Иерархические отношения между узлами, отражающие декомпозицию или детализацию понятий, реализуются с помощью связей (Relationships) типа :DECOMPOSITION. Эти связи являются направленными, указывая от более общего (родительского) узла к более частному (дочернему). Например, в базе знаний по Dungeons & Dragons, используемой для демонстрации, категория "Classes" связана отношением :DECOMPOSITION с узлами "Martial Classes" и "Magical Classes", которые, в свою очередь, декомпозируются на конкретные классы персонажей, такие как "Fighter" или "Wizard", рисунок \ref{screen-n4-classes}.

Первичная загрузка данных в базу знаний на текущем этапе разработки осуществляется путем выполнения заранее подготовленных Cypher-скриптов. Эти скрипты содержат команды для создания узлов и связей, формирующих начальную структуру и наполнение БЗ. Обновление и модификация БЗ также производятся посредством выполнения Cypher-запросов. В перспективе, для более удобного управления знаниями, предполагается разработка специализированного административного интерфейса.

\subsubsection{Реализация хранилища контекстной информации}

Для корректной работы итеративного алгоритма поиска и обеспечения осмысленности диалога с LLM-ассистентом необходимо эффективное управление контекстной информацией в рамках каждой сессии обработки запроса. В текущей MVP-реализации контекст сессии полностью инкапсулирован и хранится в оперативной памяти экземпляра класса Context на протяжении всего жизненного цикла обработки одного пользовательского запроса.

Такой подход выбран в силу его простоты и высокой производительности, так как доступ к данным в памяти осуществляется очень быстро, что важно для итеративного характера поискового процесса. Поскольку в текущей реализации Telegram-бота каждый новый текстовый запрос от пользователя, как правило, инициирует создание нового экземпляра Context, сложная персистентность контекста между независимыми запросами не является первоочередной задачей.

Хотя хранение контекста в памяти экземпляра класса Context является достаточным для текущих задач, в более сложных сценариях, предполагающих длительные или прерываемые диалоги, а также для кеширования, может потребоваться внедрение внешнего in-memory хранилища, такого как Redis. Это позволило бы сохранять состояние диалога между несколькими независимыми обращениями пользователя или между различными сессиями.

\subsection{Реализация ядра интеллектуальной обработки запросов}

Центральным элементом разработанной системы является интеллектуальное ядро, отвечающее за обработку пользовательских запросов. В отличие от традиционных поисковых систем или чисто генеративных языковых моделей, данное ядро реализует итеративный, управляемый LLM, навигационный поиск по структурированной графовой базе знаний. Основная цель этого подхода – не просто найти релевантную информацию, но сделать это максимально прозрачно и объяснимо для пользователя, предоставляя возможность уточнения контекста через диалоговое взаимодействие, если это необходимо. Ключевым компонентом, инкапсулирующим всю логику этого сложного процесса, является класс Context, экземпляры которого управляют каждой сессией обработки запроса от начала и до конца.

\subsubsection{Реализация алгоритма последовательного интеллектуального поиска}

Основной алгоритм навигации по графу знаний и принятия решений выполняется итеративно. Каждая итерация представляет собой шаг "погружения" в базу знаний до тех пор, пока не будет найден окончательный ответ или не будет достигнут установленный предел глубины поиска.

Процесс начинается с получения исходного запроса пользователя и максимального количества разрешенных итераций. При первом шаге, когда информация о предыдущих этапах отсутствует, система выполняет начальный поиск в базе знаний Neo4j, извлекая узлы верхнего уровня иерархии. На последующих итерациях, если языковая модель на предыдущем шаге рекомендовала определенные узлы для дальнейшего исследования, ядро извлекает следующий приоритетный узел из этого списка и выполняет более сфокусированный поиск. Этот поиск начинается от указанного узла и запрашивает его дочерние и связанные элементы, при этом исключая уже посещенные ранее узлы во избежание зацикливания.

Полученные из базы знаний узлы, представляющие собой фрагменты знаний, преобразуются в специальный текстовый формат. Этот формат, включающий идентификатор узла, его полный путь в иерархии, заголовок, описание и информацию о наличии дочерних элементов, передается в качестве контекста языковой модели. Затем формируется полный промпт для LLM, который также включает исходный запрос пользователя, историю предыдущих взаимодействий с LLM в текущей сессии и, возможно, комментарий LLM с предыдущего шага.

После отправки запроса к LLM и получения ответа в формате JSON, система анализирует его. На основе этого ответа принимается решение о дальнейших действиях. Если LLM указывает на конкретный идентификатор узла как на содержащий достаточную информацию для ответа, этот идентификатор сохраняется, и соответствующий путь из базы знаний добавляется в историю пройденных путей, после чего цикл поиска завершается. Если же LLM возвращает список идентификаторов узлов для дальнейшего исследования, эти узлы, после необходимой фильтрации, добавляются в очередь для обработки на следующих итерациях. Также сохраняются любые указания на вспомогательные знания и текстовый комментарий от LLM, объясняющий его текущее решение. На каждом шаге идентификаторы узлов, которые были представлены LLM для анализа, фиксируются как посещенные. Хотя текущая реализация ядра не инициирует прямых запросов к пользователю для уточнения, сам механизм итеративного анализа и возможность LLM указать в своем комментарии на нехватку информации создают основу для диалогового взаимодействия, которое может быть реализовано на уровне внешнего адаптера.

\subsubsection{Интеграция с языковыми моделями (LLM)}

Интеграция с языковыми моделями является фундаментальной для интеллектуального ядра, однако их роль строго определена: LLM выступают в качестве интеллектуальных навигаторов и анализаторов, а не как генераторы конечного текстового ответа. Технически взаимодействие с LLM осуществляется через специализированный клиент, который отправляет запросы к OpenAI-совместимому API, используя заранее сконфигурированную модель и эндпоинт. Ключ API для доступа к сервису LLM безопасно извлекается из переменных окружения. При каждом обращении к LLM формируется структурированный массив сообщений, который включает системный промпт, историю предыдущих обменов в текущей сессии (для поддержания контекста анализа) и текущий пользовательский запрос вместе с предоставленными фрагментами знаний из базы.

Ключевую роль в управлении поведением LLM играет системный промпт. Он тщательно разработан для того, чтобы инструктировать LLM выполнять строго определенные задачи: анализировать запрос пользователя относительно предоставленных фрагментов знаний и возвращать ответ исключительно в виде структурированного JSON-объекта. Этот JSON должен содержать поля, указывающие на идентификатор узла с окончательным ответом (если таковой найден и LLM в нем уверен), идентификаторы вспомогательных узлов, список идентификаторов узлов для дальнейшего исследования и текстовый комментарий, объясняющий логику LLM. Особенно важной является инструкция, предписывающая LLM всегда стремиться к более глубокому анализу, если это может привести к более полному и точному ответу, и не предлагать окончательное решение при наличии лишь поверхностной или неполной информации. Таким образом, LLM не генерирует текст ответа самостоятельно, а предоставляет ядру системы "указания" для дальнейшей навигации по базе знаний или для формирования ответа на основе уже найденного верифицированного фрагмента.

\subsubsection{Формирование и объяснение ответов}

После завершения итеративного поиска, ядро системы приступает к формированию окончательного ответа для пользователя. Этот процесс имеет две основные ветки логики в зависимости от исхода поисковой сессии.

В случае успешного нахождения релевантного знания, когда системой был определен идентификатор узла с достаточной информацией, из базы знаний Neo4j запрашивается полная информация об этом узле. Затем формируется специальный финальный промпт для LLM. В этот промпт передается исходный запрос пользователя, точное содержание найденного узла, опционально – содержание вспомогательных узлов, и, что очень важно, все накопленные комментарии LLM, которые описывают логику поиска на каждом предыдущем шаге. Системные инструкции для этого финального этапа строго предписывают дословно процитировать предоставленное содержание найденного узла без каких-либо домыслов или перефразирований. После цитаты LLM должен, на основе переданных ему комментариев о ходе поиска, кратко изложить этапы, которые привели к данному ответу. Дополнительно, если у найденного узла в базе знаний имеются прямые дочерние узлы, информация о них также извлекается и может быть добавлена к ответу в раздел "Дополнительная информация", предоставляя пользователю возможность для дальнейшего самостоятельного изучения темы.

Если же поиск завершился неудачно (например, был достигнут установленный лимит итераций без нахождения ответа), также используется финальный промпт для LLM. Однако в этом случае в качестве основного знания передается специальная информация о том, что ответ не найден, с указанием причины (например, исчерпание попыток). LLM, опираясь на накопленные комментарии о безуспешных попытках поиска, формирует объяснение, почему системе не удалось найти релевантный ответ.

Важнейшим аспектом является объяснимость ответа. Помимо текстового объяснения от LLM, система предоставляет пользователю информацию о пути навигации по базе знаний. Этот путь формируется на основе истории перемещений по графу знаний, зафиксированной во время поиска. Специальная утилита обрабатывает эту историю, создавая единый и логичный маршрут от корневого узла до найденного решения (или до точки, где поиск был остановлен). Этот путь может быть отображен пользователю через интерфейс адаптера, наглядно демонстрируя, как именно система пришла к предложенному ответу и какие разделы базы знаний были задействованы в процессе.

\subsubsection{Реализация интеллектуальных механизмов автоматизации}

Хотя разработанное интеллектуальное ядро спроектировано как универсальный компонент и не содержит логики, жестко привязанной к специфике "обработки обращений", его архитектура и функциональность предоставляют мощные инструменты для автоматизации процессов именно в этой предметной области.

Например, если в результате поиска ядро успешно определяет идентификатор узла, содержащего решение типичной проблемы или ответ на часто задаваемый вопрос, система управления обращениями может автоматически предоставить этот ответ пользователю, инициировавшему обращение, без необходимости привлечения оператора. В случаях, когда LLM на начальных этапах поиска предлагает список узлов для дальнейшего исследования, эта информация может быть использована для интеллектуальной классификации и маршрутизации входящего обращения. В зависимости от тематики первых предложенных узлов, обращение может быть автоматически направлено в соответствующий отдел поддержки или передано экспертной группе с нужной специализацией.

Механизм итеративного поиска, в рамках которого LLM может указывать на нехватку информации через свои комментарии или предлагая специфические узлы для дальнейшего изучения, позволяет системе управления обращениями автоматически запрашивать у пользователя дополнительные сведения. Это помогает собрать всю необходимую информацию для точного определения проблемы до того, как обращение, если это потребуется, будет эскалировано на оператора.

Сам механизм эскалации также становится более интеллектуальным. Если ядро завершает свою работу, сигнализируя о невозможности найти релевантное знание в базе после установленного числа попыток, это служит четким триггером для передачи обращения оператору. При этом оператору предоставляется не просто исходный текст обращения, а вся накопленная в ходе автоматического поиска информация: исходный запрос, полная история комментариев LLM, описывающая предпринятые попытки анализа, и итоговый путь навигации по базе знаний. Такой обогащенный контекст значительно сокращает время, необходимое оператору для погружения в суть проблемы и поиска эффективного решения.

В перспективе, на базе этого универсального ядра могут быть реализованы и другие интеллектуальные функции, специфичные для системы обработки обращений. К ним можно отнести проактивное предложение нескольких вариантов решений, если однозначный ответ не найден, или использование обратной связи от пользователей для динамической корректировки релевантности узлов знаний и улучшения качества работы системы в целом.

\subsection{Реализация пользовательских интерфейсов (Адаптеры)}

Для взаимодействия конечных пользователей с разработанным интеллектуальным ядром необходимы специализированные программные компоненты – адаптеры. Основная задача адаптера заключается в обеспечении удобного и привычного для пользователя интерфейса для отправки запросов и получения ответов от системы, а также в трансляции этих взаимодействий в формат, понятный ядру, и обратно. Архитектура ядра изначально проектировалась с учетом необходимости легкой интеграции с различными типами таких адаптеров, будь то мессенджеры, веб-приложения или API-сервисы. Это достигается за счет четко определенной точки входа в ядро и стандартизированного протокола обмена данными с ним, что позволяет разрабатывать новые адаптеры с минимальными усилиями и без модификации самого ядра.

\subsubsection{Реализация адаптера для Telegram-бота}

В рамках текущей разработки в качестве основного демонстрационного пользовательского интерфейса был реализован адаптер в виде Telegram-бота. Выбор Telegram обусловлен его широкой популярностью, удобным API для разработчиков и возможностью быстрого прототипирования диалоговых систем.

Логика работы Telegram-адаптера построена на последовательном взаимодействии с интеллектуальным ядром системы. При поступлении текстового сообщения от пользователя, адаптер инициирует новую сессию обработки в ядре, передавая ему содержание пользовательского запроса. После этого адаптер запускает итеративный процесс поиска, последовательно запрашивая у ядра выполнение следующего шага анализа до тех пор, пока ядро не сообщит о нахождении окончательного ответа или о достижении установленного предела глубины поиска. По завершении этого цикла обработки, адаптер получает от ядра финальный, уже отформатированный для пользователя ответ, и передает его обратно в чат Telegram. Важным аспектом является также предоставление пользователю информации о процессе принятия решения: адаптер запрашивает у ядра метаданные о пути навигации по базе знаний, который привел к найденному ответу, и отображает эту информацию, обеспечивая прозрачность и объяснимость работы системы, рисунок \ref{screen-tg-main}.

\pic[10cm]{images/screen-tg-main.png}{Пример поиска по базе знаний через Telegarm-бот API}{screen-tg-main}

Таким образом, Telegram-бот выступает в роли тонкого клиента, делегируя всю сложную логику обработки запроса и поиска информации интеллектуальному ядру. Это демонстрирует гибкость архитектуры и простоту интеграции ядра с различными каналами взаимодействия. В будущем аналогичные адаптеры могут быть разработаны для других платформ, например, для веб-интерфейса или голосовых ассистентов, используя тот же самый унифицированный API ядра.

\subsection{Тестирование и отладка}

*   Описание процесса тестирования, если будет...

*   Примеры тестовых сценариев для ключевых моментов (скрины тг?)

\subsection{Результаты разработки и демонстрация работы}

*   Финальное описание разработанной системы

*   Демонстрация работы системы на примерах со скринами/логами ()

*   Обсуждение достигнутых показателей, например увеличени скорости обработки типовых запросов, точность прогнозирования и тд. (накиунть какой-нибудь статистики)


\subsection{Вывод по разделу}

*   Резюме ключевых аспектов реализации

*   Подтверждение соответствия реализованной системы проектным решениям и требованиям

*   Оценка готовности к дальнейшему развитию
